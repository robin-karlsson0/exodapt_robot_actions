<?xml version="1.0" encoding="UTF-8"?>
<launch>
    <arg name="action_server_name" default="reply_action_server" description="Name of the action server"/>
    <arg name="reply_action_topic" default="/reply_action" description="Topic where reply action results are published"/>
    <arg name="log_pred_io_pth" default="" description="Directory path where LLM prediction IO (input, output) will be logged as individual JSON files"/>
    <arg name="inference_server_type" default="tgi" description="Type of inference server: 'tgi' or 'vllm'"/>
    <arg name="inference_server_url" default="http://localhost:8000" description="Inference server URL for LLM inference"/>
    <arg name="max_tokens" default="1024" description="Maximum tokens for reply generation"/>
    <arg name="llm_temp" default="0.6" description="Temperature for LLM next-token output distribution"/>
    <arg name="llm_seed" default="14" description="Seed for reproducible LLM results"/>

    <node pkg="reply_action" exec="reply_action" name="reply_action" output="screen" emulate_tty="true">
        <param name="action_server_name" value="$(var action_server_name)"/>
        <param name="reply_action_topic" value="$(var reply_action_topic)"/>
        <param name="log_pred_io_pth" value="$(var log_pred_io_pth)"/>
        <param name="inference_server_type" value="$(var inference_server_type)"/>
        <param name="inference_server_url" value="$(var inference_server_url)"/>
        <param name="max_tokens" value="$(var max_tokens)"/>
        <param name="llm_temp" value="$(var llm_temp)"/>
        <param name="llm_seed" value="$(var llm_seed)"/>
    </node>
</launch>